{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dataprep\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('cleaned_loan_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=['loan_status'])\n",
    "y = df['loan_status']\n",
    "\n",
    "# Split data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data and transform both sets\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for readability\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-15 16:59:38,312] A new study created in memory with name: no-name-3dd3c1fa-fd06-4957-b00a-fd1e039fa04d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-15 16:59:45,935] Trial 0 finished with value: 0.93 and parameters: {'model': 'CatBoost', 'cb_iterations': 186, 'cb_depth': 9, 'cb_learning_rate': 0.1607536366145697}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 16:59:48,901] Trial 1 finished with value: 0.91 and parameters: {'model': 'CatBoost', 'cb_iterations': 170, 'cb_depth': 4, 'cb_learning_rate': 0.08067182156572957}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 16:59:50,065] Trial 2 finished with value: 0.86 and parameters: {'model': 'Random Forest', 'rf_n_estimators': 138, 'rf_max_depth': 9}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 16:59:51,422] Trial 3 finished with value: 0.88 and parameters: {'model': 'CatBoost', 'cb_iterations': 55, 'cb_depth': 6, 'cb_learning_rate': 0.044106940517464194}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 16:59:51,442] Trial 4 finished with value: 0.21 and parameters: {'model': 'MLP', 'mlp_hidden_1': 131}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 16:59:54,485] Trial 5 finished with value: 0.92 and parameters: {'model': 'CatBoost', 'cb_iterations': 170, 'cb_depth': 4, 'cb_learning_rate': 0.09680325837682802}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 16:59:55,309] Trial 6 finished with value: 0.795 and parameters: {'model': 'Random Forest', 'rf_n_estimators': 137, 'rf_max_depth': 5}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 16:59:56,377] Trial 7 finished with value: 0.845 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 58, 'xgb_max_depth': 8, 'xgb_learning_rate': 0.02775416335832428}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 16:59:56,387] Trial 8 pruned. \n",
      "[I 2025-01-15 16:59:57,013] Trial 9 finished with value: 0.795 and parameters: {'model': 'Random Forest', 'rf_n_estimators': 106, 'rf_max_depth': 5}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 16:59:57,051] Trial 10 finished with value: 0.535 and parameters: {'model': 'Decision Tree', 'dt_max_depth': 3, 'dt_min_samples_split': 4}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 16:59:57,068] Trial 11 finished with value: 0.6 and parameters: {'model': 'Logistic Regression', 'lr_C': 0.38878446708760234}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:08,922] Trial 12 finished with value: 0.92 and parameters: {'model': 'CatBoost', 'cb_iterations': 198, 'cb_depth': 10, 'cb_learning_rate': 0.187673073467235}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:13,198] Trial 13 finished with value: 0.915 and parameters: {'model': 'CatBoost', 'cb_iterations': 153, 'cb_depth': 10, 'cb_learning_rate': 0.15232530992453008}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:13,996] Trial 14 finished with value: 0.905 and parameters: {'model': 'CatBoost', 'cb_iterations': 121, 'cb_depth': 3, 'cb_learning_rate': 0.1191013717019581}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:14,350] Trial 15 finished with value: 0.93 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 191, 'xgb_max_depth': 3, 'xgb_learning_rate': 0.19620515960221036}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:14,716] Trial 16 finished with value: 0.91 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 197, 'xgb_max_depth': 3, 'xgb_learning_rate': 0.19939210116924475}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:15,060] Trial 17 finished with value: 0.925 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 195, 'xgb_max_depth': 3, 'xgb_learning_rate': 0.19805645153173612}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:15,073] Trial 18 finished with value: 0.605 and parameters: {'model': 'Logistic Regression', 'lr_C': 0.014153349203882379}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:15,137] Trial 19 finished with value: 0.765 and parameters: {'model': 'Decision Tree', 'dt_max_depth': 10, 'dt_min_samples_split': 10}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:15,573] Trial 20 finished with value: 0.92 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 132, 'xgb_max_depth': 6, 'xgb_learning_rate': 0.11331355659592747}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:15,906] Trial 21 finished with value: 0.925 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 188, 'xgb_max_depth': 3, 'xgb_learning_rate': 0.19669016143267085}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:16,327] Trial 22 finished with value: 0.92 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 159, 'xgb_max_depth': 5, 'xgb_learning_rate': 0.15194439070705235}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:16,671] Trial 23 finished with value: 0.91 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 197, 'xgb_max_depth': 3, 'xgb_learning_rate': 0.15198925546991093}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:17,076] Trial 24 finished with value: 0.915 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 154, 'xgb_max_depth': 5, 'xgb_learning_rate': 0.159324639367187}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:17,610] Trial 25 finished with value: 0.905 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 97, 'xgb_max_depth': 10, 'xgb_learning_rate': 0.042665010621346186}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:17,661] Trial 26 finished with value: 0.65 and parameters: {'model': 'Decision Tree', 'dt_max_depth': 6, 'dt_min_samples_split': 2}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:17,674] Trial 27 finished with value: 0.6 and parameters: {'model': 'Logistic Regression', 'lr_C': 9.058501673565948}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:17,689] Trial 28 pruned. \n",
      "[I 2025-01-15 17:00:18,819] Trial 29 finished with value: 0.905 and parameters: {'model': 'CatBoost', 'cb_iterations': 105, 'cb_depth': 8, 'cb_learning_rate': 0.19502909196874446}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:19,195] Trial 30 finished with value: 0.9 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 167, 'xgb_max_depth': 4, 'xgb_learning_rate': 0.0873459689324304}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:19,566] Trial 31 finished with value: 0.905 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 196, 'xgb_max_depth': 3, 'xgb_learning_rate': 0.19574800032684175}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:19,942] Trial 32 finished with value: 0.915 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 176, 'xgb_max_depth': 4, 'xgb_learning_rate': 0.1815785225203239}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:20,255] Trial 33 finished with value: 0.91 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 181, 'xgb_max_depth': 3, 'xgb_learning_rate': 0.17466571491316868}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:20,622] Trial 34 finished with value: 0.9 and parameters: {'model': 'Random Forest', 'rf_n_estimators': 50, 'rf_max_depth': 10}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:20,953] Trial 35 finished with value: 0.92 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 143, 'xgb_max_depth': 4, 'xgb_learning_rate': 0.12581776037121636}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:22,898] Trial 36 finished with value: 0.925 and parameters: {'model': 'CatBoost', 'cb_iterations': 197, 'cb_depth': 8, 'cb_learning_rate': 0.14514425530864664}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:22,910] Trial 37 pruned. \n",
      "[I 2025-01-15 17:00:23,314] Trial 38 finished with value: 0.92 and parameters: {'model': 'XGBoost', 'xgb_n_estimators': 113, 'xgb_max_depth': 8, 'xgb_learning_rate': 0.17441819112786225}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:24,036] Trial 39 finished with value: 0.735 and parameters: {'model': 'Random Forest', 'rf_n_estimators': 193, 'rf_max_depth': 3}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:24,939] Trial 40 finished with value: 0.865 and parameters: {'model': 'CatBoost', 'cb_iterations': 79, 'cb_depth': 8, 'cb_learning_rate': 0.0143058014646434}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:26,833] Trial 41 finished with value: 0.925 and parameters: {'model': 'CatBoost', 'cb_iterations': 197, 'cb_depth': 8, 'cb_learning_rate': 0.1466989432366418}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:28,045] Trial 42 finished with value: 0.93 and parameters: {'model': 'CatBoost', 'cb_iterations': 150, 'cb_depth': 7, 'cb_learning_rate': 0.15362981732900105}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:29,067] Trial 43 finished with value: 0.915 and parameters: {'model': 'CatBoost', 'cb_iterations': 147, 'cb_depth': 6, 'cb_learning_rate': 0.17146524059117013}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:30,987] Trial 44 finished with value: 0.92 and parameters: {'model': 'CatBoost', 'cb_iterations': 144, 'cb_depth': 9, 'cb_learning_rate': 0.11994301352132697}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:31,039] Trial 45 finished with value: 0.765 and parameters: {'model': 'Decision Tree', 'dt_max_depth': 10, 'dt_min_samples_split': 9}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:32,393] Trial 46 finished with value: 0.93 and parameters: {'model': 'CatBoost', 'cb_iterations': 168, 'cb_depth': 7, 'cb_learning_rate': 0.16572968272733685}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:33,745] Trial 47 finished with value: 0.92 and parameters: {'model': 'CatBoost', 'cb_iterations': 172, 'cb_depth': 7, 'cb_learning_rate': 0.16875066792795812}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:35,206] Trial 48 finished with value: 0.915 and parameters: {'model': 'CatBoost', 'cb_iterations': 175, 'cb_depth': 7, 'cb_learning_rate': 0.16274982187801657}. Best is trial 0 with value: 0.93.\n",
      "[I 2025-01-15 17:00:36,058] Trial 49 finished with value: 0.915 and parameters: {'model': 'CatBoost', 'cb_iterations': 133, 'cb_depth': 5, 'cb_learning_rate': 0.13329917540976818}. Best is trial 0 with value: 0.93.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Best Model and Hyperparameters:\n",
      "Best Model: CatBoost\n",
      "Best Accuracy: 0.9300\n",
      "Best Hyperparameters: {'model': 'CatBoost', 'cb_iterations': 186, 'cb_depth': 9, 'cb_learning_rate': 0.1607536366145697}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Simulate a synthetic dataset for demonstration (replace with your actual dataset)\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 20)\n",
    "y = np.random.choice([0, 1], size=1000, p=[0.95, 0.05])  # Imbalanced binary target\n",
    "\n",
    "# Stratified split to ensure both classes are present in train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Handling class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Standardize the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the objective function for Optuna optimization\n",
    "def objective(trial):\n",
    "    model_name = trial.suggest_categorical('model',\n",
    "                                           ['Decision Tree', 'Logistic Regression', 'Random Forest', 'XGBoost', 'CatBoost', 'MLP'])\n",
    "\n",
    "    # **Decision Tree (CPU Only)**\n",
    "    if model_name == 'Decision Tree':\n",
    "        model = DecisionTreeClassifier(\n",
    "            max_depth=trial.suggest_int('dt_max_depth', 3, 10),\n",
    "            min_samples_split=trial.suggest_int('dt_min_samples_split', 2, 10)\n",
    "        )\n",
    "\n",
    "    # **Logistic Regression (CPU Only)**\n",
    "    elif model_name == 'Logistic Regression':\n",
    "        model = LogisticRegression(\n",
    "            C=trial.suggest_loguniform('lr_C', 0.01, 10),\n",
    "            solver='liblinear'\n",
    "        )\n",
    "\n",
    "    # **Random Forest (CPU Only)**\n",
    "    elif model_name == 'Random Forest':\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=trial.suggest_int('rf_n_estimators', 50, 200),\n",
    "            max_depth=trial.suggest_int('rf_max_depth', 3, 10)\n",
    "        )\n",
    "\n",
    "    # **XGBoost with GPU**\n",
    "    elif model_name == 'XGBoost':\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=trial.suggest_int('xgb_n_estimators', 50, 200),\n",
    "            max_depth=trial.suggest_int('xgb_max_depth', 3, 10),\n",
    "            learning_rate=trial.suggest_float('xgb_learning_rate', 0.01, 0.2),\n",
    "            use_label_encoder=False,\n",
    "            eval_metric=\"logloss\",\n",
    "            tree_method=\"gpu_hist\"\n",
    "        )\n",
    "\n",
    "    # **CatBoost with GPU**\n",
    "    elif model_name == 'CatBoost':\n",
    "        # Check if multiple classes exist\n",
    "        if len(set(y_train_resampled)) < 2:\n",
    "            raise ValueError(\"Target variable contains only one class. Check data balancing.\")\n",
    "        \n",
    "        model = CatBoostClassifier(\n",
    "            iterations=trial.suggest_int('cb_iterations', 50, 200),\n",
    "            depth=trial.suggest_int('cb_depth', 3, 10),\n",
    "            learning_rate=trial.suggest_float('cb_learning_rate', 0.01, 0.2),\n",
    "            task_type=\"GPU\",\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "    # **MLP with PyTorch (GPU)**\n",
    "    elif model_name == 'MLP':\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(X_train_scaled.shape[1], trial.suggest_int('mlp_hidden_1', 50, 200)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(trial.suggest_int('mlp_hidden_1', 50, 200), 1),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        loss_fn = nn.BCELoss()\n",
    "\n",
    "        # Move data to GPU\n",
    "        X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "        y_train_tensor = torch.tensor(y_train_resampled, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Simple training loop with pruning\n",
    "        for epoch in range(3):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train_tensor).flatten()\n",
    "            loss = loss_fn(outputs, y_train_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Report progress for pruning\n",
    "            trial.report(loss.item(), step=epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Move test data to GPU for evaluation\n",
    "        X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Predict using PyTorch model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = (model(X_test_tensor).flatten() > 0.5).cpu().numpy()\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        return accuracy\n",
    "\n",
    "    #For all other models (Scikit-Learn)\n",
    "    model.fit(X_train_scaled, y_train_resampled)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "#Run Optuna study with pruning enabled\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "#Display the best model and hyperparameters\n",
    "print(\"\\nðŸŽ¯ Best Model and Hyperparameters:\")\n",
    "print(f\"Best Model: {study.best_params['model']}\")\n",
    "print(f\"Best Accuracy: {study.best_value:.4f}\")\n",
    "print(f\"Best Hyperparameters: {study.best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x1c219d42680>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Retrain the best model with the entire dataset\n",
    "best_model = CatBoostClassifier(\n",
    "    iterations=study.best_params['cb_iterations'],\n",
    "    depth=study.best_params['cb_depth'],\n",
    "    learning_rate=study.best_params['cb_learning_rate'],\n",
    "    task_type=\"GPU\",\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "best_model.fit(X_train_scaled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved as 'best_catboost_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model to a file\n",
    "joblib.dump(best_model, \"best_catboost_model.pkl\")\n",
    "print(\"Best model saved as 'best_catboost_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model Accuracy: 0.9300\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model and test it on unseen data\n",
    "loaded_model = joblib.load(\"best_catboost_model.pkl\")\n",
    "\n",
    "# Test the loaded model\n",
    "y_pred_loaded = loaded_model.predict(X_test_scaled)\n",
    "loaded_accuracy = accuracy_score(y_test, y_pred_loaded)\n",
    "print(f\"Loaded Model Accuracy: {loaded_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df))         # Should be <class 'pandas.core.frame.DataFrame'>\n",
    "print(type(X_train_scaled))  # Should be <class 'numpy.ndarray'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Extract feature names from the original DataFrame\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Get feature importance from the CatBoost model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m feature_importance \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mget_feature_importance()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract feature names from the original DataFrame\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# Get feature importance from the CatBoost model\n",
    "feature_importance = best_model.get_feature_importance()\n",
    "\n",
    "# Sort the features by importance for better visualization\n",
    "sorted_idx = feature_importance.argsort()\n",
    "sorted_feature_names = [feature_names[i] for i in sorted_idx]\n",
    "\n",
    "# Plot feature importance using actual column names\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(sorted_feature_names, feature_importance[sorted_idx])\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature Name\")\n",
    "plt.title(\"CatBoost Feature Importance with Actual Feature Names\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
